{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning process: train and test a DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Based on following example from TensorFLow\n",
    "\n",
    "Title: Timeseries classification from scratch  \n",
    "Author: [hfawaz](https://github.com/hfawaz/)  \n",
    "Date created: 2020/07/21  \n",
    "Last modified: 2021/07/16  \n",
    "Description: Training a timeseries classifier from scratch on the FordA dataset from the UCR/UEA archive.  \n",
    "\n",
    "This example shows how to do timeseries classification from scratch, starting from raw\n",
    "CSV timeseries files on disk. We demonstrate the workflow on the FordA dataset from the\n",
    "[UCR/UEA archive](https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for Papermill\n",
    "\n",
    "dataset_name = 'BasicMotions' # FordA FordB ArrowHead BasicMotions SelfRegulationSCP1 SelfRegulationSCP2 df7_20220710_023405 df8_20220721_182341 df9_20220722_065855(etc)\n",
    "classifier_name = 'cnn' # cnn fcn resnet11\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/Users/stevemead/LISAandNEA/\"\n",
    "\n",
    "timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "run_id = f'{dataset_name}_{classifier_name}_{timestamp}'\n",
    "output_directory = f'{root_dir}results/{run_id}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(directory_path):\n",
    "    '''Create a new directory, returns None if directory_path already exists or an error occurs'''\n",
    "    if os.path.exists(directory_path):\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(directory_path)\n",
    "        except:\n",
    "            return None\n",
    "        return directory_path\n",
    "\n",
    "def get_output_file_name(output_file, file_type):\n",
    "    return f'{output_directory}/{output_file}_{run_id}.{file_type}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ucr_datasets(filename):\n",
    "    data = np.loadtxt(filename)\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Here we visualize one timeseries example for each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in ['FordA','FordB','ArrowHead']:\n",
    "    \n",
    "    print('Univariate UCR TSC dataset')\n",
    "    # Load the data\n",
    "\n",
    "    x_train, y_train = read_ucr_datasets(f'{root_dir}datasets/{dataset_name}/{dataset_name}_TRAIN.txt')\n",
    "    x_test, y_test = read_ucr_datasets(f'{root_dir}datasets/{dataset_name}/{dataset_name}_TEST.txt')\n",
    "\n",
    "    # Visualise the data\n",
    "\n",
    "    print(type(x_train))\n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "    \n",
    "    print(y_train[:100])\n",
    "    classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
    "\n",
    "    plt.figure()\n",
    "    for c in classes:\n",
    "        c_x_train = x_train[y_train == c]\n",
    "        plt.plot(c_x_train[0], label=\"class \" + str(c))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Standardise the data (see comments below re: znormalise)\n",
    "\n",
    "    # Reshape the data\n",
    "\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "elif dataset_name in ['BasicMotions','SelfRegulationSCP1','SelfRegulationSCP2']:\n",
    "    print('Multivariate UEA TSC dataset')\n",
    "    \n",
    "    # Load the data\n",
    "    from sktime.datasets import load_from_tsfile\n",
    "    \n",
    "    x_train, y_train = load_from_tsfile(\n",
    "        f'{root_dir}datasets/{dataset_name}/{dataset_name}_TRAIN.ts', return_data_type=\"numpy3d\")\n",
    "    x_test, y_test = load_from_tsfile(\n",
    "        f'{root_dir}datasets/{dataset_name}/{dataset_name}_TEST.ts', return_data_type=\"numpy3d\")\n",
    "\n",
    "    # Visualise the data\n",
    "    print(type(x_train))\n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "    x_train = np.transpose(x_train, (0,2,1))\n",
    "    x_test = np.transpose(x_test, (0,2,1))\n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "    \n",
    "    print(y_train[:100])\n",
    "\n",
    "    max_xyz, max_aet = 0,0\n",
    "    for i in range(0,3):\n",
    "        max_val = abs(x_train[:,:,i]).max()\n",
    "        if max_val>max_xyz: max_xyz = max_val\n",
    "    for i in range(3,6):\n",
    "        max_val = abs(x_train[:,:,i]).max()\n",
    "        if max_val>max_aet: max_aet = max_val\n",
    "    x_train[:,:,0:3] = x_train[:,:,0:3]/max_xyz\n",
    "    x_train[:,:,3:6] = x_train[:,:,3:6]/max_aet\n",
    "    x_test[:,:,0:3] = x_test[:,:,0:3]/max_xyz\n",
    "    x_test[:,:,3:6] = x_test[:,:,3:6]/max_aet\n",
    "\n",
    "    classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
    "\n",
    "    \n",
    "    for c in classes:\n",
    "        c_x_train = x_train[y_train == c]\n",
    "        print(c_x_train.shape)\n",
    "        plt.figure()\n",
    "        plt.plot(c_x_train[0,:,0:3], label=\"class \" + str(c))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        plt.figure()\n",
    "        plt.plot(c_x_train[0,:,3:6], label=\"class \" + str(c))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # Standardise the data (see comments below re: znormalise)\n",
    "\n",
    "else:\n",
    "    print('Multivariate LISAandNEA TSC dataset')\n",
    "\n",
    "    # load the data\n",
    "    df_name = run_id.split('_')[0]\n",
    "    print(df_name)\n",
    "    \n",
    "    # load y\n",
    "    df_y_file_path = f'{root_dir}datasets/{df_name}.txt'\n",
    "    df_y = pd.read_csv(df_y_file_path)\n",
    "    all_y = df_y['type'].to_numpy()\n",
    "    # y_train = df_y['type'][:2500]\n",
    "    # y_test = df_y['type'][2500:]\n",
    "    \n",
    "    # load x\n",
    "    signals = 'fluctuations'\n",
    "    all_x_file_path = f'{root_dir}datasets/{dataset_name}/dataset_{signals}_{dataset_name}.npy'\n",
    "    all_x = np.load(all_x_file_path)*1e5\n",
    "    # x_train = all_x[:2500,:,:]\n",
    "    # x_test = all_x[2500:,:,:]\n",
    "\n",
    "    # split the dataset into training and testing, keeping same proportions of each class in both\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        all_x, all_y, test_size=1/6, random_state=1702, shuffle=False\n",
    "    )\n",
    "    x_train = all_x[:2500,:,:]\n",
    "    x_test = all_x[2500:,:,:]\n",
    "    y_train = all_y[:2500]\n",
    "    y_test = all_y[2500:]\n",
    "\n",
    "    print(y_train.shape, x_train.shape, y_test.shape, x_test.shape)\n",
    "\n",
    "    # rescale the data so that the maximum absolute value of each feature is scaled to unit size\n",
    "\n",
    "    max_xyz, max_aet = 0,0\n",
    "    for i in range(0,3):\n",
    "        max_val = abs(x_train[:,:,i]).max()\n",
    "        if max_val>max_xyz: max_xyz = max_val\n",
    "    for i in range(3,6):\n",
    "        max_val = abs(x_train[:,:,i]).max()\n",
    "        if max_val>max_aet: max_aet = max_val\n",
    "    x_train[:,:,0:3] = x_train[:,:,0:3]/max_xyz\n",
    "    x_train[:,:,3:6] = x_train[:,:,3:6]/max_aet\n",
    "    x_test[:,:,0:3] = x_test[:,:,0:3]/max_xyz\n",
    "    x_test[:,:,3:6] = x_test[:,:,3:6]/max_aet\n",
    "    \n",
    "    # scalers = {}\n",
    "    # for i in range(x_train.shape[2]):\n",
    "    #     scalers[i] = sklearn.preprocessing.MaxAbsScaler()\n",
    "    #     x_train[:,:,i] = scalers[i].fit_transform(x_train[:,:,i])\n",
    "    #     print(f'Scale of scaler[{i}]:', max(scalers[i].scale_))\n",
    "    \n",
    "    # for i in range(x_test.shape[2]):\n",
    "    #     x_test[:,:,i] = scalers[i].transform(x_test[:,:,i])\n",
    "\n",
    "    classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
    "    print(classes)\n",
    "    \n",
    "    for c in classes:\n",
    "        c_x_train = x_train[y_train == c]\n",
    "        print(c_x_train.shape)\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(c_x_train[0,:,0:3], label=\"class \" + str(c))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(c_x_train[0,:,3:6], label=\"class \" + str(c))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "        plt.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.max(), x_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, x_train.shape, y_test.shape, x_test.shape)\n",
    "print(type(y_train), type(x_train), type(y_test), type(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "np.unique(np.concatenate((y_train, y_test), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "y_train.shape, y_test.shape, y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y_test_original = y_test\n",
    "\n",
    "if (len(y_train.shape) == 1) & (len(y_test.shape) == 1):\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# y_ datasets have shape (number_of_samples, number_of_classes)\n",
    "# Class 0 (or -1 in the original value) is the first class. Index 0 along axis=1\n",
    "# Class 1 is the second class. Index 1 along axis=1\n",
    "y_train.shape, y_test.shape, y_train[:10], enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(y_true.shape, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.testing.assert_array_equal(y_true, y_test_original)\n",
    "    print('y_true is a perfect match for y_test_original')\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model or 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cnn_model(input_shape, num_classes):\n",
    "\n",
    "    padding = 'valid'\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=6,kernel_size=7,padding=padding,activation='sigmoid')(input_layer)\n",
    "    conv1 = keras.layers.AveragePooling1D(pool_size=3)(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=12,kernel_size=7,padding=padding,activation='sigmoid')(conv1)\n",
    "    conv2 = keras.layers.AveragePooling1D(pool_size=3)(conv2)\n",
    "\n",
    "    flatten_layer = keras.layers.Flatten()(conv2)\n",
    "\n",
    "    output_layer = keras.layers.Dense(units=num_classes,activation='sigmoid')(flatten_layer)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fcn_model(input_shape, num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation('relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=128, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap_layer)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet11_model(input_shape, num_classes):\n",
    "\n",
    "    n_feature_maps = 64\n",
    "\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    # BLOCK 1\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "    # BLOCK 3\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # no need to expand channels because they are equal\n",
    "    shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "    output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "\n",
    "    # FINAL\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap_layer)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_file = get_output_file_name('training_log','csv')\n",
    "init_model_file = get_output_file_name('init_model','h5')\n",
    "best_model_file = get_output_file_name('best_model','h5')\n",
    "last_model_file = get_output_file_name('last_model','h5')\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    best_model_file, save_best_only=True, monitor=\"val_loss\")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=50, min_lr=0.0001)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=50, verbose=1)\n",
    "\n",
    "csv_logger = keras.callbacks.CSVLogger(\n",
    "    filename=training_log_file, separator=\",\", append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        N = 50\n",
    "        if (epoch+1)%N == 0:  # save once every N epochs\n",
    "            self.model.save(get_output_file_name(f'mid_model_{epoch:04d}','h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "callbacks = [saver, model_checkpoint, csv_logger]\n",
    "lr_logged = True\n",
    "validation_split = 0.2\n",
    "# epochs is now a papermill parameter\n",
    "batch_size = 16\n",
    "\n",
    "if classifier_name == 'cnn':\n",
    "    model = make_cnn_model(\n",
    "        input_shape=input_shape, num_classes=num_classes)\n",
    "    model.compile(\n",
    "        loss='mean_squared_error', \n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    lr_logged = False\n",
    "elif classifier_name == 'fcn':\n",
    "    callbacks.append(reduce_lr)\n",
    "    model = make_fcn_model(\n",
    "        input_shape=input_shape, num_classes=num_classes)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    batch_size = int(min(x_train.shape[0]/10, 16))\n",
    "elif classifier_name == 'resnet11':\n",
    "    callbacks.append(reduce_lr)\n",
    "    model = make_resnet11_model(input_shape=input_shape, num_classes=num_classes)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    # epochs = 1500\n",
    "    batch_size = int(min(x_train.shape[0]/10, 64))\n",
    "else:\n",
    "    print(f'Model type {classifier_name} unsupported')\n",
    "\n",
    "model.save_weights(init_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fig = get_output_file_name('model','png')\n",
    "keras.utils.plot_model(model, show_shapes=True, rankdir='TB', to_file=model_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_about_file = get_output_file_name('df_about','csv')\n",
    "\n",
    "df_about = pd.DataFrame(data=np.zeros((1, 2), dtype=int), index=[0],\n",
    "                                 columns=['epochs', 'batch_size'])\n",
    "\n",
    "df_about['epochs'] = epochs\n",
    "df_about['batch_size'] = batch_size\n",
    "df_about['validation_split'] = validation_split\n",
    "df_about['run_id'] = run_id\n",
    "df_about['dataset_name'] = dataset_name\n",
    "df_about['classifier_name'] = classifier_name\n",
    "df_about['timestamp'] = timestamp\n",
    "df_about['lr_logged'] = lr_logged\n",
    "df_about['num_instances_train'] = x_train.shape[0] # total training instances, then split into training and validation datasets\n",
    "df_about['num_instances_test'] = x_test.shape[0] # total testing instance, not used for training or validation\n",
    "df_about['num_samples'] = x_train.shape[1]\n",
    "df_about['num_variables'] = x_train.shape[2]\n",
    "df_about['num_classes'] = num_classes\n",
    "\n",
    "df_about.to_csv(df_about_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "fit_start_time = time()\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=validation_split,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "fit_duration = time() - fit_start_time\n",
    "model.save(last_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "print(f'Trained {classifier_name} on {dataset_name} in {str(timedelta(seconds=int(fit_duration)))} ({int(fit_duration)} seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_file = get_output_file_name('history','csv')\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(history_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_model_file = get_output_file_name('df_best_model','csv')\n",
    "\n",
    "index_best_model = history_df['val_loss'].idxmin()\n",
    "row_best_model = history_df.loc[index_best_model]\n",
    "df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=float), index=[0],\n",
    "                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n",
    "                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n",
    "\n",
    "df_best_model['best_model_train_loss'] = row_best_model['loss']\n",
    "df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n",
    "df_best_model['best_model_train_acc'] = row_best_model['accuracy']\n",
    "df_best_model['best_model_val_acc'] = row_best_model['val_accuracy']\n",
    "if lr_logged:\n",
    "    df_best_model['best_model_learning_rate'] = row_best_model['lr']\n",
    "df_best_model['best_model_nb_epoch'] = index_best_model\n",
    "\n",
    "df_best_model.to_csv(df_best_model_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model's training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(epochs, history, metric, bpe=None, zoomed=None, save_figure_as=None, show=False):\n",
    "    '''Plot the history of loss over training epochs'''\n",
    "    fig, axs = plt.subplots(nrows=1, \n",
    "                        ncols=1, \n",
    "                        # sharex='col', \n",
    "                        # sharey='row', \n",
    "                        squeeze=False, \n",
    "                        figsize=(6, 4))\n",
    "    xs = np.arange(1,epochs+1,1)\n",
    "    plt.plot(xs, history[metric], label=f'Training {metric}', zorder=2)\n",
    "    plt.plot(xs, history['val_' + metric], label=f'Validation {metric}', zorder=1)\n",
    "    if bpe is not None:\n",
    "        axs[0,0].axvline(x=bpe, ymin=0.0, ymax=0.95, color='purple', ls='--', label='Best performing epoch', zorder=0) \n",
    "    axs[0,0].set(xlabel='Epoch', ylabel=metric.capitalize())\n",
    "    if metric=='loss':\n",
    "        axs[0,0].legend(loc='upper right')\n",
    "    else:\n",
    "        axs[0,0].legend(loc='lower right')\n",
    "        axs[0,0].set(ylim=(0,1.05))\n",
    "    if zoomed is not None:\n",
    "        axs[0,0].set(xlim=zoomed) \n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if save_figure_as is not None:\n",
    "        plot_history_file = get_output_file_name(save_figure_as,'png')\n",
    "        fig.patch.set_alpha(1)\n",
    "        fig.savefig(plot_history_file)\n",
    "    plt.close()\n",
    "\n",
    "plot_history(epochs, history.history, metric='loss', save_figure_as='epochs_loss')\n",
    "# plot_history(200, history.history, metric='loss', zoomed=(0-5,200+5), save_figure_as='epochs_loss_zoom200')\n",
    "plot_history(epochs, history.history, metric='accuracy', save_figure_as='epochs_accuracy')\n",
    "# plot_history(200, history.history, metric='accuracy', zoomed=(0-5,200+5), save_figure_as='epochs_accuracy_zoom200')\n",
    "\n",
    "plot_history(epochs, history.history, metric='loss', bpe=index_best_model+1, save_figure_as='epochs_loss_bpe', show=True)\n",
    "plot_history(epochs, history.history, metric='accuracy', bpe=index_best_model+1, save_figure_as='epochs_accuracy_bpe', show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(best_model_file)\n",
    "\n",
    "evaluation = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_eval_file = get_output_file_name('df_metrics_eval','csv')\n",
    "\n",
    "df_metrics_eval = pd.DataFrame(data=np.zeros((1, len(model.metrics_names)), dtype=float), index=[0],\n",
    "                       columns=model.metrics_names)\n",
    "\n",
    "for i, metric in enumerate(model.metrics_names):\n",
    "    df_metrics_eval[metric] = evaluation[i]\n",
    "    print(f'Test {metric}: {evaluation[i]}')\n",
    "\n",
    "df_metrics_eval.to_csv(df_metrics_eval_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# raw probabilities to chosen class (highest probability)\n",
    "y_pred = np.argmax(y_pred,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape, y_pred.shape, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_test_file = get_output_file_name('df_metrics_test','csv')\n",
    "\n",
    "df_metrics_test = pd.DataFrame(data=np.zeros((1, 4), dtype=float), index=[0],\n",
    "                       columns=['precision', 'accuracy', 'recall', 'duration'])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "df_metrics_test['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "df_metrics_test['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "df_metrics_test['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "df_metrics_test['duration'] = fit_duration\n",
    "\n",
    "df_metrics_test.to_csv(df_metrics_test_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ROC curve and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot an ROC. pred - the predictions, y - the expected output.\n",
    "def plot_roc(pred, y, save_figure_as=None):\n",
    "    fpr, tpr, _ = roc_curve(y, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    if save_figure_as is not None:\n",
    "        roc_file = get_output_file_name(save_figure_as,'png')\n",
    "        fig.patch.set_alpha(1)\n",
    "        fig.savefig(roc_file)\n",
    "    plt.show()\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix_original(cm, names, title='Confusion matrix', \n",
    "                            cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_classes == 2: # univariate only\n",
    "    plot_roc(y_pred, y_true, save_figure_as='roc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.testing.assert_array_equal(y_true, y_pred)\n",
    "    print('Predictions are a perfect match for ground truth!')\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_file = get_output_file_name('df_cm','csv')\n",
    "\n",
    "df_cm = pd.DataFrame(data=cm)\n",
    "\n",
    "df_cm.to_csv(df_cm_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_norm_file = get_output_file_name('df_cm_norm','csv')\n",
    "\n",
    "df_cm_norm = pd.DataFrame(data=cm_normalized)\n",
    "\n",
    "df_cm_norm.to_csv(df_cm_norm_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True,\n",
    "                          save_figure_as=None):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    fig = plt.figure(figsize=(4.5, 4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    # plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    if save_figure_as is not None:\n",
    "        cm_file = get_output_file_name(save_figure_as,'png')\n",
    "        fig.patch.set_alpha(1)\n",
    "        fig.savefig(cm_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix')\n",
    "print(cm)\n",
    "plot_confusion_matrix(cm, enc.categories_[0], \n",
    "        normalize=False, save_figure_as='cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "plot_confusion_matrix(cm_normalized, enc.categories_[0], \n",
    "        save_figure_as='cm_norm')\n",
    "# plot_confusion_matrix_original(cm_normalized, enc.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the testing data (last 500 rows) out of the file used to define the dataset\n",
    "df_y_test = df_y.tail(500)\n",
    "\n",
    "# Append the true and predicted values\n",
    "# Integer values correspond to classes in alphabetical order: \n",
    "#   0 = glitch\n",
    "#   1 = gwburst\n",
    "#   2 = nea\n",
    "\n",
    "df_y_test['y_true'] = y_true\n",
    "df_y_test['y_pred'] = y_pred\n",
    "\n",
    "# Save the testing data for posterity\n",
    "df_y_test_file = get_output_file_name('df_y_test','csv')\n",
    "df_y_test.to_csv(df_y_test_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca41972b84f72b46829c2c393ce8a96594463a1f5472ad57fca1cf73a6cd343f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
